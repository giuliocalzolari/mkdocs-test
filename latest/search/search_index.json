{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome my docs","text":"<p>Version is MASTER</p>"},{"location":"backup/intro/","title":"intro backup","text":""},{"location":"backup/todo/","title":"todo backup","text":""},{"location":"installation/intro/","title":"intro to install","text":""},{"location":"installation/quick-start-install-aws/","title":"Quick S AWS","text":"<p>The AWS Load Balancer controller (LBC) provisions AWS Network Load Balancer (NLB) and Application Load Balancer (ALB) resources. The LBC watches for new <code>service</code> or <code>ingress</code> Kubernetes resources and configures AWS resources.</p> <p>The LBC is supported by AWS. Some clusters may be using the legacy \"in-tree\" functionality to provision AWS load balancers. The AWS Load Balancer Controller should be installed instead.</p> <p>!!!question \"Existing AWS ALB Ingress Controller users\"     The AWS ALB Ingress controller must be uninstalled before installing the AWS Load Balancer Controller.     Please follow our migration guide to do a migration.</p> <p>!!!warning \"When using AWS Load Balancer Controller v2.5+\"     The AWS LBC provides a mutating webhook for service resources to set the <code>spec.loadBalancerClass</code> field for service of type LoadBalancer on create.      This makes the AWS LBC the default controller for service of type LoadBalancer. You can disable this feature and revert to set Cloud Controller Manager (in-tree controller) as the default by setting the helm chart value enableServiceMutatorWebhook to false with <code>--set enableServiceMutatorWebhook=false</code> .      You will no longer be able to provision new Classic Load Balancer (CLB) from your kubernetes service unless you disable this feature. Existing CLB will continue to work fine.</p>"},{"location":"installation/quick-start-install-aws/#supported-kubernetes-versions","title":"Supported Kubernetes versions","text":"<ul> <li>AWS Load Balancer Controller v2.0.0~v2.1.3 requires Kubernetes 1.15+</li> <li>AWS Load Balancer Controller v2.2.0~v2.3.1 requires Kubernetes 1.16-1.21</li> <li>AWS Load Balancer Controller v2.4.0+ requires Kubernetes 1.19+</li> <li>AWS Load Balancer Controller v2.5.0+ requires Kubernetes 1.22+</li> </ul>"},{"location":"installation/quick-start-install-aws/#deployment-considerations","title":"Deployment considerations","text":""},{"location":"installation/quick-start-install-aws/#additional-requirements-for-non-eks-clusters","title":"Additional requirements for non-EKS clusters:","text":"<ul> <li>Ensure subnets are tagged appropriately for auto-discovery to work</li> <li>For IP targets, pods must have IPs from the VPC subnets. You can configure the <code>amazon-vpc-cni-k8s</code> plugin for this purpose.</li> </ul>"},{"location":"installation/quick-start-install-aws/#additional-requirements-for-isolated-cluster","title":"Additional requirements for isolated cluster:","text":"<p>Isolated clusters are clusters without internet access, and instead reply on VPC endpoints for all required connects. When installing the AWS LBC in isolated clusters, you need to disable shield, waf and wafv2 via controller flags <code>--enable-shield=false, --enable-waf=false, --enable-wafv2=false</code></p>"},{"location":"installation/quick-start-install-aws/#using-the-amazon-ec2-instance-metadata-server-version-2-imdsv2","title":"Using the Amazon EC2 instance metadata server version 2 (IMDSv2)","text":"<p>We recommend blocking the access to instance metadata by requiring the instance to use IMDSv2 only. For more information, please refer to the AWS guidance here. If you are using the IMDSv2, set the hop limit to 2 or higher in order to allow the LBC to perform the metadata introspection. </p> <p>You can set the IMDSv2 as follows:</p> <pre><code>aws ec2 modify-instance-metadata-options --http-put-response-hop-limit 2 --http-tokens required --region &lt;region&gt; --instance-id &lt;instance-id&gt;\n</code></pre> <p>Instead of depending on IMDSv2, you can specify the AWS Region via the controller flag <code>--aws-region</code>, and the AWS VPC via controller flag <code>--aws-vpc-id</code> or by specifying vpc tags via the flag <code>--aws-vpc-tags</code> and an optional flag <code>--aws-vpc-tag-key</code> if you have a different key for the tag other than \"Name\". When both flags <code>--aws-vpc-id</code> and <code>--aws-vpc-tags</code> are specified, the controller prioritizes <code>--aws-vpc-id</code>and ignores the other flag.</p>"},{"location":"installation/quick-start-install-aws/#configure-iam","title":"Configure IAM","text":"<p>The controller runs on the worker nodes, so it needs access to the AWS ALB/NLB APIs with IAM permissions.</p> <p>The IAM permissions can either be setup using IAM roles for service accounts (IRSA) or can be attached directly to the worker node IAM roles. The best practice is using IRSA if you're using Amazon EKS. If you're using kOps or self-hosted Kubernetes, you must manually attach polices to node instances.</p>"},{"location":"installation/quick-start-install-aws/#option-a-recommended-iam-roles-for-service-accounts-irsa","title":"Option A: Recommended, IAM roles for service accounts (IRSA)","text":"<p>The reference IAM policies contain the following permissive configuration:</p> <pre><code>{\n    \"Effect\": \"Allow\",\n    \"Action\": [\n        \"ec2:AuthorizeSecurityGroupIngress\",\n        \"ec2:RevokeSecurityGroupIngress\"\n    ],\n    \"Resource\": \"*\"\n},\n</code></pre> <p>We recommend further scoping down this configuration based on the VPC ID or cluster name resource tag.</p> <p>Example condition for VPC ID:</p> <pre><code>    \"Condition\": {\n        \"ArnEquals\": {\n            \"ec2:Vpc\": \"arn:aws:ec2:&lt;REGION&gt;:&lt;ACCOUNT-ID&gt;:vpc/&lt;VPC-ID&gt;\"\n        }\n    }\n</code></pre> <p>Example condition for cluster name resource tag:</p> <pre><code>    \"Condition\": {\n        \"Null\": {\n            \"aws:ResourceTag/kubernetes.io/cluster/&lt;CLUSTER-NAME&gt;\": \"false\"\n        }\n    }\n</code></pre> <ol> <li> <p>Create an IAM OIDC provider. You can skip this step if you already have one for your cluster.     <code>eksctl utils associate-iam-oidc-provider \\         --region &lt;region-code&gt; \\         --cluster &lt;your-cluster-name&gt; \\         --approve</code></p> </li> <li> <p>Download an IAM policy for the LBC using one of the following commands:<p>     If your cluster is in a US Gov Cloud region:     <code>curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.7.0/docs/install/iam_policy_us-gov.json</code>     If your cluster is in a China region:     <code>curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.7.0/docs/install/iam_policy_cn.json</code>     If your cluster is in any other region:     <code>curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.7.0/docs/install/iam_policy.json</code></p> <li> <p>Create an IAM policy named <code>AWSLoadBalancerControllerIAMPolicy</code>. If you downloaded a different policy, replace <code>iam-policy</code> with the name of the policy that you downloaded.     <code>aws iam create-policy \\         --policy-name AWSLoadBalancerControllerIAMPolicy \\         --policy-document file://iam-policy.json</code>     Take note of the policy ARN that's returned.</p> </li> <li> <p>Create an IAM role and Kubernetes <code>ServiceAccount</code> for the LBC. Use the ARN from the previous step.     <code>eksctl create iamserviceaccount \\     --cluster=&lt;cluster-name&gt; \\     --namespace=kube-system \\     --name=aws-load-balancer-controller \\     --attach-policy-arn=arn:aws:iam::&lt;AWS_ACCOUNT_ID&gt;:policy/AWSLoadBalancerControllerIAMPolicy \\     --override-existing-serviceaccounts \\     --region &lt;region-code&gt; \\     --approve</code></p> </li>"},{"location":"installation/quick-start-install-aws/#option-b-attach-iam-policies-to-nodes","title":"Option B: Attach IAM policies to nodes","text":"<p>If you're not setting up IAM roles for service accounts, apply the IAM policies from the following URL at a minimum. Please be aware of the possibility that the controller permissions may be assumed by other users in a pod after retrieving the node role credentials, so the best practice would be using IRSA instead of attaching IAM policy directly.</p> <pre><code>curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.7.0/docs/install/iam_policy.json\n</code></pre> <p>The following IAM permissions subset is for those using <code>TargetGroupBinding</code> only and don't plan to use the LBC to manage security group rules:</p> <pre><code>{\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"ec2:DescribeVpcs\",\n                \"ec2:DescribeSecurityGroups\",\n                \"ec2:DescribeInstances\",\n                \"elasticloadbalancing:DescribeTargetGroups\",\n                \"elasticloadbalancing:DescribeTargetHealth\",\n                \"elasticloadbalancing:ModifyTargetGroup\",\n                \"elasticloadbalancing:ModifyTargetGroupAttributes\",\n                \"elasticloadbalancing:RegisterTargets\",\n                \"elasticloadbalancing:DeregisterTargets\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"*\"\n        }\n    ],\n    \"Version\": \"2012-10-17\"\n}\n</code></pre>"},{"location":"installation/quick-start-install-aws/#network-configuration","title":"Network configuration","text":"<p>Review the worker nodes security group docs. Your node security group must permit incoming traffic on TCP port 9443 from the Kubernetes control plane. This is needed for webhook access.</p> <p>If you use eksctl, this is the default configuration.</p> <p>If you use custom networking, please refer to the EKS Best Practices Guides for network configuration.</p>"},{"location":"installation/quick-start-install-aws/#add-controller-to-cluster","title":"Add controller to cluster","text":"<p>We recommend using the Helm chart to install the controller. The chart supports Fargate and facilitates updating the controller.</p> <p>=== \"Helm\"</p> <pre><code>If you want to run the controller on Fargate, use the Helm chart, since it doesn't depend on the `cert-manager`.\n\n### Detailed instructions\nFollow the instructions in the [aws-load-balancer-controller](https://github.com/aws/eks-charts/tree/master/stable/aws-load-balancer-controller) Helm chart.\n\n### Summary\n\n1. Add the EKS chart repo to Helm\n```\nhelm repo add eks https://aws.github.io/eks-charts\n```\n2. If upgrading the chart via `helm upgrade`, install the `TargetGroupBinding` CRDs.\n```\nwget https://raw.githubusercontent.com/aws/eks-charts/master/stable/aws-load-balancer-controller/crds/crds.yaml\nkubectl apply -f crds.yaml\n```\n\n    !!!tip\n        The `helm install` command automatically applies the CRDs, but `helm upgrade` doesn't.\n\n\nHelm install command for clusters with IRSA:\n```\nhelm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=&lt;cluster-name&gt; --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller\n```\n\nHelm install command for clusters not using IRSA:\n```\nhelm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=&lt;cluster-name&gt;\n```\n</code></pre> <p>=== \"YAML manifests\"</p> <pre><code>### Install `cert-manager`\n\n```\nkubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.12.3/cert-manager.yaml\n```\n\n### Apply YAML\n1. Download the spec for the LBC.\n```\nwget https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.7.0/v2_7_0_full.yaml\n```\n2. Edit the saved yaml file, go to the Deployment spec, and set the controller `--cluster-name` arg value to your EKS cluster name\n```\napiVersion: apps/v1\nkind: Deployment\n. . .\nname: aws-load-balancer-controller\nnamespace: kube-system\nspec:\n    . . .\n    template:\n        spec:\n            containers:\n                - args:\n                    - --cluster-name=&lt;your-cluster-name&gt;\n```\n3. If you use IAM roles for service accounts, we recommend that you delete the `ServiceAccount` from the yaml spec. If you delete the installation section from the yaml spec, deleting the `ServiceAccount` preserves the `eksctl` created `iamserviceaccount`.\n```\napiVersion: v1\nkind: ServiceAccount\n```\n4. Apply the yaml file\n```\nkubectl apply -f v2_7_0_full.yaml\n```\n5. Optionally download the default ingressclass and ingressclass params\n```\nwget https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.7.0/v2_7_0_ingclass.yaml\n```\n6. Apply the ingressclass and params\n```\nkubectl apply -f v2_7_0_ingclass.yaml\n```\n</code></pre>"},{"location":"installation/quick-start-install-aws/#create-update-strategy","title":"Create Update Strategy","text":"<p>The controller doesn't receive security updates automatically. You need to manually upgrade to a newer version when it becomes available.</p> <p>You can upgrade using <code>helm upgrade</code> or another strategy to manage the controller deployment.</p>"},{"location":"installation/quick-start-install-azure/","title":"Quick S Azure","text":""},{"location":"installation/quick-start-install-generic/","title":"Quick S Gen","text":""},{"location":"installation/quick-start-install-google/","title":"Quick S Google","text":""},{"location":"installation/requirements-aws/","title":"Req AWS","text":""},{"location":"installation/requirements-azure/","title":"Req Azure","text":""},{"location":"installation/requirements-generic/","title":"Req Gen","text":""},{"location":"installation/requirements-google/","title":"Req Google","text":""},{"location":"installation/verification/","title":"verify that","text":""},{"location":"migration/intro/","title":"intro Monitoring","text":""},{"location":"migration/todo/","title":"todo Monitoring","text":""},{"location":"monitoring/intro/","title":"intro monitoring","text":""},{"location":"monitoring/todo/","title":"todo monitoring","text":"<p>I did it</p> <pre><code>apiVersion: test\nkind: Test\nmetadata:\n  name: test\nspec:\n  foo: bar\n</code></pre>"}]}